# get date of latest entree

# search parameters
date_month=$(journalctl -u student_project_backup.service |tail -1 | cut -d " " -f 1)
date_day=$(journalctl -u student_project_backup.service |tail -1 | cut -d " " -f 2)
date=$(date --date="$(printf "01 %s"$mydate)" +"%Y-%m"-$date_day )
date1=$(date --date="next day" +%y-%m-%d)
timestamp=$(date +%s%N)

T_archives=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Number of Archives on {{ ansible_nodename }}" | tail -1 | rev | cut -d " " -f 1 | rev)

N_success=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep success | wc -l)
exit_status=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep successfully | wc -l)
N_pruning=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Pruning archive" | wc -l)
N_databases_created=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Archive name:" |wc -l)
N_fail_error=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep error |wc -l)

Original_size_GB=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Size archives on {{ ansible_nodename }}" | tail -1 | xargs |  cut -d ":" -f 7 |  cut -d " " -f 2| cut -d "." -f 1)
Compressed_size_GB=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Size archives on {{ ansible_nodename }}" | tail -1 | xargs |  cut -d ":" -f 7 |  cut -d " " -f 4| cut -d "." -f 1)
Deduplicated_size_GB=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Size archives on {{ ansible_nodename }}" | tail -1 | xargs |  cut -d ":" -f 7 |  cut -d " " -f 6| cut -d "." -f 1)

Perc_compressed_size=$(echo "$Compressed_size_GB/1000" | bc -l | awk '{printf "%f", $0}')
echo "borg_backup,Host={{ ansible_nodename }} Pruned_archives=${N_pruning}i,Number_of_databases_created=${N_databases_created}i,succesfull_jobs=${N_success}i,errors=${N_fail_error}i,total_archives=${T_archives}i,exit_status=${exit_status}i,original_size=${Original_size_GB}i,compressed_size=${Compressed_size_GB}i,de_duplicated_size=${Deduplicated_size_GB}i,Perc_compressed_size_gb=${Perc_compressed_size} $timestamp"

T_archives=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Number of Archives on {{ backup_archive_addres_1 }}" | tail -1 | rev | cut -d " " -f 1 | rev)

N_success0
exit_status=1
N_pruning=0
N_databases_created=0
N_fail_error=0

Original_size_GB=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Size archives on {{ backup_archive_addres_1 }}" | tail -1 | xargs |  cut -d ":" -f 7 |  cut -d " " -f 2| cut -d "." -f 1)
Compressed_size_GB=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Size archives on {{ backup_archive_addres_1 }}" | tail -1 | xargs |  cut -d ":" -f 7 |  cut -d " " -f 4| cut -d "." -f 1)
Deduplicated_size_GB=$(journalctl -u student_project_backup.service --since=$date  --until=$date1 | grep "Size archives on {{ backup_archive_addres_1 }}" | tail -1 | xargs |  cut -d ":" -f 7 |  cut -d " " -f 6| cut -d "." -f 1)

Perc_compressed_size=$(echo "$Compressed_size_GB/1000" | bc -l | awk '{printf "%f", $0}') 
echo "borg_backup,Host={{ backup_archive_addres_1 }} Pruned_archives=${N_pruning}i,Number_of_databases_created=${N_databases_created}i,succesfull_jobs=${N_success}i,errors=${N_fail_error}i,total_archives=${T_archives}i,exit_status=${exit_status}i,original_size=${Original_size_GB}i,compressed_size=${Compressed_size_GB}i,de_duplicated_size=${Deduplicated_size_GB}i,Perc_compressed_size_gb=${Perc_compressed_size} $timestamp"
